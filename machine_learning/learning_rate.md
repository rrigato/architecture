- small learning rates increase training time because they represent how far you are moving in the direction of your learning function for each epoch
- decreasing the learning rate prevents overshooting the global minima