- activation functions = defines output of a node based on the input to that node

- rectified linear unit (RELU) = creates sparse output, more efficient and handles the vanishing gradient problem better than tanh/sigmoid
- maxout = outputs the max of the input
- softmax = final output layer that can provide multiple class category probabilities 
- sigmoid (logistic) = can provide multi-label predictions